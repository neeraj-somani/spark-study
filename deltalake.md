# Top links to understand basics of Delta lake:
- Apache Hudi, Apache Iceberg, and Delta Lake [here](https://chat.openai.com/share/ccbdd232-c646-42e8-ac0d-7fd5947c7d7b)
- Copy on Write(CoW) and Merge On Read(MoR)

## few delta related spark settings
- MERGE operations support generated columns when you set spark.databricks.delta.schema.autoMerge.enabled to true.
- to set the delta.appendOnly = true property for all new Delta Lake tables created in a session -- SET spark.databricks.delta.properties.defaults.appendOnly = true
- spark.conf.set("spark.databricks.delta.replaceWhere.constraintCheck.enabled", False)
  - to replace the matching rows in the target table, you can disable the constraint check by setting
- SET spark.sql.sources.partitionOverwriteMode=dynamic;
  - Configure dynamic partition overwrite mode
  - Delta Lake 2.0 and above supports dynamic partition overwrite mode for partitioned tables
  - we overwrite all existing data in each logical partition for which the write will commit new data. Any existing logical partitions for which the write does not contain data will remain unchanged.
  - Dynamic partition overwrite conflicts with the option replaceWhere for partitioned tables.
      - If dynamic partition overwrite is enabled in the Spark session configuration, and replaceWhere is provided as a DataFrameWriter option, then Delta Lake overwrites the data according to the replaceWhere expression (query-specific options override session configurations).
      - Youâ€™ll receive an error if the DataFrameWriter options have both dynamic partition overwrite and replaceWhere enabled.
